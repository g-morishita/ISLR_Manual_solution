---
title: "Manual_Solution_Exercise3"
author: "Gota Morishita"
date: "9/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Cenceptual

## 1
The null hypothese corresponding to TABLE 3.4 are 

* TV advertisement has no association with sales.
* radio advertisement has no association with sales.
* newspaper advertisement has no association with sales.

Looking at TABLE 3.4, TV and radio has low p-values, so TV and radio have nonnegligible association with sales. On the other hand, newspaper does not association with sales.

## 2
KNN classifier is used for classification problem as you can guess from its name. The way to do that is to gather $K$ points closest to a point you want to estimate and assign the point to the most common class among the $K$ nearest points.

On the other hand, KNN regressoin is used for regression problem. The estimation procedure is similar to KNN classifier. First, gather $K$ points closest to a point you want to estimate and assign the estimated point to averaged values of $K$ observed response variables.

## 3
The linear model is as follows:

$$
salary = 50 + 20 \times GPA + 0.07 \times IQ + 35 \times Gender + 0.01 \times GPA \times IQ - 10 \times GPA \times Gender
$$

### (a)
The correct answer is (iii).

With IQ and GPA fixed, $salary = (35 - 10 \times GPA) \times Gender + const.$. When GPA is high enough, the coefficient of Gender is negative, so males earn more money than female since the coding of Gender is 1 for female and 0 for male.

### (b)

Substituting 1, 110 and 4 for Gender, IQ, and GPA, we have $salary = 50 + 80 + 7.7 + 35 + 4.4 - 40 = 137.1$

### (c)
False. Small value of a coefficient does not mean little evidence of an effect while small p-value does.

## 4

### (a)
We expect the cubic regression to have lower training RSS because there is a noise when you observe the data and the cubic regression is more complex, thus fitting the training data better than the linear regression.

### (b)
We expect the linear regression to have lower test RSS. The cubic regression tends to fit the observed data too well to generalize. 

### (c)
The linear regression is a submodel of the cubic regression. Therefore, the training RSS of the cubic regression is always smaller than that of the linear regression.

### (d)
There is not enough information to tell which model has lower training RSS. It depends on the true model generating the data.

## 5

$$
a_{i'} = \frac{x_{i'}x_i}{\sum_kx_k^2}
$$

## 6
From (3.4), we have $\bar{y} = \hat{\beta}_0 + \hat{\beta}_1\bar{x}$, which completes the proof.

## 7

Assume that $\bar{y} = \bar{x} = 0$.

$$ \begin{aligned} RSS &= \sum_{i=1}^{n} \ (y_i-\hat{y}_{i})^2 \\  TSS &= \sum_{i=1}^{n} \ y_i^2\end{aligned}$$
Our aim is to show that $R^2 = Cor(X,Y)^2$

$$ 
\begin{aligned} R^2 &= 1 - RSS/TSS \\
  &= 1 - \frac{\sum(y_i -\hat{y}_{i})^2}{\sum(y_i)^2} \\
  \hat{y}_{i} &= \hat{\beta_1}x_i \\
  &= \frac{\sum x_iy_i}{\sum x_i^2}
\end{aligned}
$$

Substituting $\hat{y_i}$, we have 

$$
\begin{aligned} 
R^2 &= \frac{\sum x_iy_i}{\sum x_i^2 \sum y_i^2}
\end{aligned}
$$

# Applied

Set up
```{r}
library(ISLR)
library(ggplot2)
```

## 8
__(a)__

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
```

### (i)
The p-value for horsepower is very low, so we can say that there is a (negative) relationship between the predictor and the response.

### (ii)
The $R^2$ statistic is 0.6059, so the relationship is moderately strong.

### (iii)
Negative.

### (iv)
```{r}
predict(lm.fit, data.frame(horsepower = c(98)), interval = 'confidence')
```

```{r}
predict(lm.fit, data.frame(horsepower = c(98)), interval = 'prediction')
```


__(b)__

```{r}
ggplot(data = Auto) + 
  geom_point(mapping = aes(x = horsepower, y = mpg)) +
  geom_smooth(mapping = aes(x = horsepower, y = mpg), method = "lm", formula = y ~ x, se=FALSE)
```


__(c)__
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```


* In Residuals vs Fitted plot, we can see the U-shape curve, which indicates the data has non-linearity.
* In Scale-location, we can see that the assumption that variance is constant through examples is liked to be violated.

## 9

__(a)__

```{r}
plot(Auto)
```

__(b)__

```{r}
cor(subset(Auto, select = -name))
```

__(c)__

```{r}
lm.fit <- lm(mpg ~ .-name, data = Auto)
summary(lm.fit)
```

* Looking at F-statistics, there is a relationship between the predictors and the response.
* R^2 statistics is 0.8215, so the linear model explains the relationship.
* displacement, weight, year, and origin have a statistically significant relationship to the response.
* the positive coefficient of `year` variable suggests newer cars are more effective.

__(d)__
```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
```

* In Residuals vs Fitted, you can see a slight non-linear trend.
* There is an observation with high leverage.

__(e)__
```{r}

```

__(f)__
We applied $X^2$ transformations to the four predictors.
```{r}
lm.fit2 <- lm(mpg ~ origin + I(origin^2) + year + I(year^2) + weight + I(weight^2) + horsepower + I(horsepower^2), data = Auto)
summary(lm.fit2)
```

```{r}
par(mfrow = c(2, 2))
plot(lm.fit2)
```
* We have higher R^2 Statistic even though some predictors are discarded.
* In Residuals vs Fitted values plot, the non-linear trend is gone.
* In scale-Location plot, it looks like the variance is constant.


## 10

__(a)__
```{r}
lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
```

__(b)__
```{r}
summary(lm.fit)
```

Looking at the p-values of the coefficients, whether a store is in an urban location or not does not have an effect on sales while whether a store is in the U.S. or not and the price have association with the sales.
If the price goes up, the sales go down. 
If a store is in the U.S., the sales go up.

__(c)__

$$
Sales = 13.043469 - 0.054459 * Price - -0.02191 * UrbanYes + 1.200573 * USYes
$$

__(d)__
Price and US

__(e)__
```{r}
small.fit <- lm(Sales ~ Price + US, data = Carseats)
summary(small.fit)
```

__(f)__

The smaller model has higher adjusted $R^2$ statistic, so the smaller one fits to the data better.

__(g)__
```{r}
confint(small.fit)
```

__(h)__
```{r}
plot(predict(small.fit), rstudent(small.fit))
```

Studentized residuals of all observations are between -3 and 3. Hence, there is no evidence that there is an outlier.

```{r}
plot(hatvalues(small.fit))
```

The average leverage for all the observations is always $(p+1) / n$. In this case, $3 / 400 = 0.075$. So, an observation with higher leverage statistic than 0.075 might be a high leverage observation.
There is an observation with leverage statistic of around 0.04. Hence, we can say that there is a high leverage observation.

## 11

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
```


__(a)__

```{r}
lm.fit <- lm(y ~ x - 1)
summary(lm.fit)
```

The t-statistic of the coefficient is so high (the p-value is so low) that the null hypothesis that the coefficient is zero.

__(b)__
```{r}
lm.fit2 <- lm(x ~ y - 1)
summary(lm.fit2)
```

The t-statistc and p-value is identical to the regression of $Y$ on $X$.

__(d)__

The model is 

$$
y = \beta x + \epsilon \quad \epsilon \sim \mathcal{N}(0, \sigma)
$$

$\widehat{\beta} = \frac{\sum x_i y_i}{\sum x_i^2}$, so

$$
Var[\widehat{\beta}] = \frac{\sum x_i^2 \sigma^2}{\big(\sum x_i^2\big)} = \frac{\sigma^2}{\sum x_i^2}
$$

$\widehat{\sigma} = \frac{RSS}{n-1} = \frac{\sum \big(y_i - \widehat{\beta}x_i\big)}{n-1}$.

Therefore, $SE[\beta] = \sqrt{\frac{\sum \big(y_i - \widehat{\beta}x_i\big)}{(n-1)\big(\sum x_i^2 \big)}}$.

Next, we calculate the t-statistic $\widehat{\beta}/SE[\widehat{\beta}]$. Substituting $\widehat{\beta}$ and $E[\widehat{\beta}]$ with the above and simplifying, we have the answer.

```{r}
t <- sqrt(length(x) - 1) * sum(x * y) / sqrt(sum(x * x) * sum(y * y) - sum(x * y) ** 2)
print(t)
```

__(e)__
The t-statistic is symmetric, so the t-statistic of the regression of $Y$ on $X$ is the same as that of the regression of $X$ on $Y$.

__(f)__

```{r}
lm.fit2 <- lm(y ~ x)
summary(lm.fit2)
```

```{r}
lm.fit3 <- lm(x ~ y)
summary(lm.fit3)
```

## 12

__(a)__
If $\sum x_i^2 = \sum y_i^2$, the estimated coefficient is the same.

__(b)__
```{r}
n <- 100
set.seed(1)
x <- rnorm(100)
y <- 3 * x + rnorm(100, 0, 0.1)
```

```{r}
lm.fit <- lm(y ~ x -1)
coef(lm.fit)
```

```{r}
lm.fit2 <- lm(x ~ y - 1)
coef(lm.fit2)
```

__(c)__

```{r}
set.seed(1)
x <- rnorm(100)
y <- sample(x, 100)
```

```{r}
lm.fit <- lm(y ~ x - 1)
coef(lm.fit)
```

```{r}
lm.fit2 <- lm(y ~ x - 1)
coef(lm.fit2)
```


## 13

```{r}
set.seed(1)
```

__(a)__

```{r}
x <- rnorm(100)
```

__(b)__

```{r}
eps <- rnorm(100, 0, 0.25)
```

__(c)__

```{r}
y <- -1 + 0.5 * x + eps
```

__(d)__

```{r}
ggplot(mapping = aes(x = x, y = y)) + 
  geom_point()
```

There is a linear relationship between y and x.

__(e)__

```{r}
lm.fit <- lm(y ~ x)
summary(lm.fit)
```

Estimated coefficients are very close to the true ones.

__(f)__

```{r}
ggplot() + 
  geom_point(mapping = aes(x = x, y = y)) +
  geom_abline(slope = 0.5, intercept = -1, show.legend =  TRUE) +
  geom_smooth(mapping = aes(x = x, y = y), method = 'lm', formula = y ~ x, color = "blue", se = FALSE)
```

__(g)__

```{r}
lm.fit2 <- lm(y ~ poly(x, 2))
summary(lm.fit2)
```

```{r}
ggplot() + 
  geom_point(mapping = aes(x = x, y = y)) +
  geom_abline(slope = 0.5, intercept = -1, show.legend =  TRUE) +
  geom_smooth(mapping = aes(x = x, y = y), method = 'lm', formula = y ~ x + I(x^2), color = "blue", se = FALSE)
```

Looking at the graph, the previous model looks better, but looking at the summary info. the quadratic model looks better because the $X^2$ coefficient has low p-value and adjusted $R^2$ statistic is higher than the previous model. 

__(h)__

```{r}
for (s in c(0.20, 0.15, 0.10, 0.05, 0.01)) {
  set.seed(1)
  x <- rnorm(100)
  eps <- rnorm(100, 0, s)
  y <- -1 + 0.5 * x + eps
  lm.fit <- lm(y ~ x)
  print(summary(lm.fit))
}
```

When the noise gets smaller, the model fitting gets better.
The estimated standard errors of the coefficients get smaller, too.

__(i)__

```{r}
for (s in c(0.25, 0.30, 0.35, 0.40, 0.45)) {
  set.seed(1)
  x <- rnorm(100)
  eps <- rnorm(100, 0, s)
  y <- -1 + 0.5 * x + eps
  lm.fit <- lm(y ~ x)
  print(summary(lm.fit))
}
```

The fitting gets worse when the noise gets bigger.

__(j)__

```{r}
x <- rnorm(100)
eps <- rnorm(100, 0, 0.25)
y <- -1 + 0.5 * x + eps
lm.fit <- lm(y ~ x)
confint(lm.fit)
```

```{r}
x <- rnorm(100)
eps <- rnorm(100, 0, 0.05)
y <- -1 + 0.5 * x + eps
lm.fit <- lm(y ~ x)
confint(lm.fit)
```

```{r}
x <- rnorm(100)
eps <- rnorm(100, 0, 0.5)
y <- -1 + 0.5 * x + eps
lm.fit <- lm(y ~ x)
confint(lm.fit)
```

## 14

__(a)__

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100, 0, 0.1)
y <- 2 + 2 * x1 + 0.3 *x2 + rnorm(100)
```

$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon,
$$
where $\beta_0 = 2, \beta_1 = 2, \beta_2 = 0.3$.

__(b)__

```{r}
ggplot() +
  geom_point(mapping = aes(x = x1, y = x2))
```

__(c)__
```{r}
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
```

The estimated intercept $\beta_0$ is close to the true one, but the coefficients $\beta_1, \beta_2$ are not so close. Also, the standard errors are so high that the p-values are not high, especially for the second coefficient $\beta_2$.

The null hypothesis $\beta_1 = 0$ can be rejected but the null hypothesis $\beta_2$ cannot be rejected.

__(d)__

```{r}
lm.fit2 <- lm(y ~ x1)
summary(lm.fit2)
```

The s.e. significantly decreases, so that the null hypothesis can be rejected.

__(e)__
```{r}
lm.fit3 <- lm(y ~ x2)
summary(lm.fit3)
```

The same for the previous question.

__(f)__
No, they don't. $X_1$ and $X_2$ are strongly correlated, so the standard errors blow up. If one of them is removed, collinearity is gone. Then, the standard error becomes small and the null hypothesis can be rejected.

__(g)__

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
lm.fit4 <- lm(y ~ x1 + x2)
summary(lm.fit4)
```

```{r}
lm.fit5 <- lm(y ~ x1)
summary(lm.fit5)
```

```{r}
lm.fit6 <- lm(y ~ x2)
summary(lm.fit6)
```

In the full model, the coefficient of $X_1$ is not statistically significant anymore. Instead $X_2$ is significant.

```{r}
plot(hatvalues(lm.fit4))
```

```{r}
plot(hatvalues(lm.fit5))
```

```{r}
plot(hatvalues(lm.fit6))
```

The added observation has high leverage in the first and third model.

```{r}
rstudent(lm.fit4)[101]
```

```{r}
rstudent(lm.fit5)[101]
```

```{r}
rstudent(lm.fit6)[101]
```

In the second model, it is considered an outlier.


## 15

__(a)__

```{r}
library(MASS)
attach(Boston)
print(names(Boston))
```

### zn
```{r}
lm.zn <- lm(crim ~ zn)
summary(lm.zn)
```

Yes.

### indus
```{r}
lm.indus <- lm(crim ~ indus)
summary(lm.indus)
```

Yes.

### chas
```{r}
lm.chas <- lm(crim ~ chas)
summary(lm.chas)
```

No

### nox
```{r}
lm.nox <- lm(crim ~ nox)
summary(lm.nox)
```

Yes

### rm
```{r}
lm.rm <- lm(crim ~ rm)
summary(lm.rm)
```

Yes.

### age
```{r}
lm.age <- lm(crim ~ age)
summary(lm.age)
```

Yes.

### dis
```{r}
lm.dis <- lm(crim ~ dis)
summary(lm.dis)
```

Yes.

### rad
```{r}
lm.rad <- lm(crim ~ rad)
summary(lm.rad)
```

Yes.

### tax
```{r}
lm.tax <- lm(crim ~ tax)
summary(lm.tax)
```

Yes

### ptratio
```{r}
lm.ptratio <- lm(crim ~ ptratio)
summary(lm.ptratio)
```

### black
```{r}
lm.black <- lm(crim ~ black)
summary(lm.black)
```

Yes.

### lstat
```{r}
lm.lstat <- lm(crim ~ lstat)
summary(lm.lstat)
```

Yes.

### medv
```{r}
lm.medv <- lm(crim ~ medv)
summary(lm.medv)
```

Yes.

All but chas are associated with the response. You can see why by looking at the correlation. The relationship between chas and crime is weak compared to the other predictors.

```{r}
cor(Boston)
```

__(b)__
```{r}
lm.all <- lm(crim ~ ., data = Boston)
summary(lm.all)
```

It depends on the significance level which predictor is statistically significant, but we can say dis, rad and medv are statistically significant.

__(c)__

```{r}
x = c(coefficients(lm.zn)[2],
      coefficients(lm.indus)[2],
      coefficients(lm.chas)[2],
      coefficients(lm.nox)[2],
      coefficients(lm.rm)[2],
      coefficients(lm.age)[2],
      coefficients(lm.dis)[2],
      coefficients(lm.rad)[2],
      coefficients(lm.tax)[2],
      coefficients(lm.ptratio)[2],
      coefficients(lm.black)[2],
      coefficients(lm.lstat)[2],
      coefficients(lm.medv)[2])
y = coefficients(lm.all)[2:14]
plot(x, y)
```

__(d)__

Omitted. Looking at the Residual vs Fitted plot, you can see if there is a non-linear trend.

```{r}
plot(lm.medv)
```






















